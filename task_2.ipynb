{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6SxhV_Cr-Dn",
    "outputId": "64fe2128-9039-4373-ba51-77d482851f0a",
    "ExecuteTime": {
     "end_time": "2024-11-04T08:12:12.713350Z",
     "start_time": "2024-11-04T08:12:11.206273Z"
    }
   },
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
   ],
   "metadata": {
    "id": "mOh3iV85sVEg"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "model_name = \"TheAIchemist13/kannada_beekeeping_wav2vec2\"\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "audio_file = \"INPUT FILE.mp3\" # path to the input file\n",
    "audio_input, _ = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "input_values = tokenizer(audio_input, return_tensors=\"pt\").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]"
   ],
   "metadata": {
    "id": "nh0zpt0VttRv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_transcriptions(transcription_file):\n",
    "    transcriptions = {}\n",
    "    with open(transcription_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            filename, transcription = line.strip().split(\": \", 1)\n",
    "            transcriptions[filename] = transcription\n",
    "    return transcriptions"
   ],
   "metadata": {
    "id": "3tjpHmUHtF_L"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "key = \"AIzaSyCLFUq-pV2Z0-6msv3iYI7EHdbQjKewN3o\"\n",
    "genai.configure(api_key=key)\n",
    "\n",
    "transcriptions = load_transcriptions(\"transcriptions.txt\")\n",
    "prompt = f\"From these transcriptions {transcriptions}, which are in the format (Filename: transcription), give a segment of the transcription to answer this question: {transcription}. Don't add any new information OR CHANGE existing text other than the provided transcriptions.\"\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n"
   ],
   "metadata": {
    "id": "tnPkH3GLseud"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "4EExAworsibm"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
